{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recupération des données\n",
    "TRAIN_SET_PATH = \"/media/sameh/data/france/madar_shared_task/MADAR-Shared-Task-Subtask-1/MADAR-Corpus-26-train.txt\"\n",
    "import pandas as pd\n",
    "\n",
    "with open(TRAIN_SET_PATH, \"r\", encoding='utf_8') as infile:\n",
    "    X, y = [], []\n",
    "    for line in infile:\n",
    "        text, label  = line.split(\"\\t\")\n",
    "        X.append(text)\n",
    "        y.append(label)\n",
    "data = {'phrase':X,'label':y}\n",
    "df= pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretraitement des données \n",
    "import re \n",
    "import tashaphyne ,sys \n",
    "import tashaphyne.arabic_const as arabcons\n",
    "def strip_tashkeel(text): \n",
    "    return tashaphyne.arabic_const.HARAKAT_PAT.sub('', text) \n",
    "def strip_tatweel(text): \n",
    "    return re.sub(tashaphyne.arabic_const.TATWEEL, '', text) \n",
    "def normalize_hamza(text):\n",
    "    text = tashaphyne.arabic_const.ALEFAT_PAT.sub(tashaphyne.arabic_const.ALEF, text) \n",
    "    return tashaphyne.arabic_const.HAMZAT_PAT.sub(tashaphyne.arabic_const.HAMZA, text) \n",
    "def normalize_lamalef(text):\n",
    "    return tashaphyne.arabic_const.LAMALEFAT_PAT.sub(\\\n",
    "                                       u'%s%s'%(tashaphyne.arabic_const.LAM, tashaphyne.arabic_const.ALEF), text) \n",
    "def normalize_spellerrors(text): \n",
    "    text = re.sub(tashaphyne.arabic_const.TEH_MARBUTA,tashaphyne.arabic_const.HEH, text) \n",
    "    return re.sub(tashaphyne.arabic_const.ALEF_MAKSURA,tashaphyne.arabic_const.YEH, text)\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | \n",
    "                             َ    | \n",
    "                             ً    | \n",
    "                             ُ    | \n",
    "                             ٌ    | \n",
    "                             ِ    | \n",
    "                             ٍ    | \n",
    "                             ْ    | \n",
    "                             ـ     \n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "listtrain=[]\n",
    "#supprime les diactritics \n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "#supprime les signes des ponctuations \n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', arabic_punctuations)\n",
    "    return text.translate(translator)\n",
    "#Supprimer les nombres \n",
    "def remove_numbers(text):\n",
    "    regex = re.compile(r\"(\\d|[\\u0660\\u0661\\u0662\\u0663\\u0664\\u0665\\u0666\\u0667\\u0668\\u0669])+\")\n",
    "    return re.sub(regex, ' ', text)\n",
    "#supprimer les noms non arabe \n",
    "def remove_non_arabic_words(text):\n",
    "    return ' '.join([word for word in text.split() if not re.findall(\n",
    "        r'[^\\s\\u0621\\u0622\\u0623\\u0624\\u0625\\u0626\\u0627\\u0628\\u0629\\u062A\\u062B\\u062C\\u062D\\u062E\\u062F\\u0630\\u0631\\u0632\\u0633\\u0634\\u0635\\u0636\\u0637\\u0638\\u0639\\u063A\\u0640\\u0641\\u0642\\u0643\\u0644\\u0645\\u0646\\u0647\\u0648\\u0649\\u064A]',\n",
    "        word)])\n",
    "train = list(df['phrase'])\n",
    "for data in train:\n",
    "    data = normalize_spellerrors(data)\n",
    "    data = normalize_lamalef(data)\n",
    "    data = normalize_hamza(data)\n",
    "    data = strip_tashkeel(data)\n",
    "    data = strip_tatweel(data)\n",
    "    data = remove_diacritics(data)\n",
    "    data = remove_punctuations(data)\n",
    "    data = remove_numbers(data)\n",
    "    data = remove_non_arabic_words(data)\n",
    "    listtrain.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['phrase']= listtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth...ng_strategy='auto')), ('multinomialnb', MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]\n",
    "#Pipeline\n",
    "text_clf = make_pipeline_imb( TfidfVectorizer(tokenizer=stemming_tokenizer,analyzer='word',ngram_range=(1,1)),\n",
    "                      RandomUnderSampler(),\n",
    "                     MultinomialNB(alpha=0.5,fit_prior=True, class_prior=None)) \n",
    "text_clf.fit(df['phrase'], df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200\n",
      "Accuracy =  0.5963461538461539\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SET_PATH = \"/media/sameh/data/france/madar_shared_task/MADAR-Shared-Task-Subtask-1/MADAR-Corpus-26-dev.tsv\"\n",
    "with open(TRAIN_SET_PATH, \"r\", encoding='utf_8') as infile:\n",
    "    xt,yt = [],[]\n",
    "    for line in infile:\n",
    "        text, label  = line.split(\"\\t\")\n",
    "        xt.append(text)\n",
    "        yt.append(label)\n",
    "#recupére donée de test dans data frame \n",
    "datatest = {'phrase':xt,'label':yt}\n",
    "dftest= pd.DataFrame(datatest)\n",
    "test = list(dftest['phrase'])\n",
    "listtest=[]\n",
    "for data in test:\n",
    "    data = normalize_spellerrors(data)\n",
    "    data = normalize_lamalef(data)\n",
    "    data = normalize_hamza(data)\n",
    "    data = strip_tashkeel(data)\n",
    "    data = strip_tatweel(data)\n",
    "    data = remove_diacritics(data)\n",
    "    data = remove_punctuations(data)\n",
    "    data = remove_numbers(data)\n",
    "    data = remove_non_arabic_words(data)\n",
    "    listtest.append(data)\n",
    "dftest['phrase']= listtest\n",
    "predicted = text_clf.predict(dftest['phrase'])\n",
    "print (len(predicted))\n",
    "print('Accuracy = ',np.mean(predicted == dftest['label']))\n",
    "#print(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import kenlm , math, glob \n",
    "import os,sys\n",
    "import pandas as pd\n",
    "#function qui return la liste de score de chaque phrase\n",
    "def language(text,code):\n",
    "    text = ' '.join('#'.join(text.split()))\n",
    "    nommodel = '/media/sameh/data/CORPUS/LM-corpus/modelm/'+ code +'.binary'\n",
    "    model=kenlm.LanguageModel(nommodel)\n",
    "    prob = math.pow(10,model.score(text))\n",
    "    #prob = math.floor(1000*prob)/1000            \n",
    "    #print('listscore',len(listscore))\n",
    "    return prob\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        res=[{'score': x[0],'model':x[1]} \n",
    "            for x in X[[ 'score','model']].values]\n",
    "        return res\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "# function qui return la meilleur score+ le nom d model \n",
    "corp = ['ale-char', 'asw-char', 'msa-char', 'sal-char', 'jed-char', 'bag-char', 'san-char', 'kha-char', 'rab-char', 'mus-char', 'jer-char', 'bas-char', 'tri-char', 'alg-char', 'riy-char', 'amm-char', 'tun-char', 'fes-char', 'ben-char', 'alx-char', 'sfx-char', 'dam-char', 'bei-char', 'cai-char', 'mos-char', 'doh-char']\n",
    "#print(models)\n",
    "def scoremodel(s):\n",
    "    m = list(map(lambda code: kenlm.LanguageModel('/media/sameh/data/CORPUS/LM-corpus/modelm/' + code + \".binary\"), corp))\n",
    "\n",
    "    s = ' '.join('#'.join(s.split()))\n",
    "    maxl = ''\n",
    "    maxp =  -sys.maxsize + 1 #minimum integer in python\n",
    "    totalp = 0.0\n",
    "    for j in range(len(m)):\n",
    "        model = m[j]\n",
    "        prob = model.score(s)\n",
    "        totalp += math.pow(10.0, prob)\n",
    "        if(prob > maxp):\n",
    "            maxp = prob\n",
    "            maxl = corp[j]\n",
    "    #if else yetna7aw et round to proba sera dans boucle for pui l'ajout dans liste  \n",
    "    if(totalp==0.0): prob = 0.0\n",
    "    else: prob = math.pow(10.0,maxp)/totalp\n",
    "    #round to thousandths\n",
    "    prob = math.floor(1000*prob)/1000\n",
    "    tupler=(maxl,prob)\n",
    "    return tupler       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41600, 4)\n"
     ]
    }
   ],
   "source": [
    "#ajouter au df 1 column pour 26 score par phrase\n",
    "listscoreph=[ ]\n",
    "listmodelph=[ ]\n",
    "for i in list(df['phrase']):\n",
    "    listscoreph.append(scoremodel(i)[1])\n",
    "    listmodelph.append(scoremodel(i)[0])\n",
    "df['score'] = listscoreph\n",
    "df['model'] = listmodelph\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  phrase  label  score  \\\n",
      "0                               امام بيانات الساءح تماما  MSA\\n  0.999   \n",
      "1              لم اسمع بهذا العنوان من قبل بالقرب من هنا  MSA\\n  0.999   \n",
      "2            استمر في السير في هذا الطريق حتي تجد صيدليه  MSA\\n  0.999   \n",
      "3                                       كم تكلفه الافطار  MSA\\n  0.521   \n",
      "4                                     كيف استطيع مساعدتك  MSA\\n  0.994   \n",
      "5                         اتجه يسارا عند الناصيه الثالثه  MSA\\n  0.999   \n",
      "6                       هل تحب ان تضع قشده وسكر في قهوتك  MSA\\n  0.999   \n",
      "7          هل يمكنكم صرف الشيك ذو الماءتي دولار الخاص بي  MSA\\n  0.999   \n",
      "8                            اذا حدث ذلك من فضلك اتصل بي  MSA\\n  0.999   \n",
      "9                                             اين المقهي  MSA\\n  0.300   \n",
      "10                                    اريد جاكيت للاطفال  MSA\\n  0.993   \n",
      "11     حسنا انها تتكلف خمسه وثمانون دولارا في البريد ...  MSA\\n  0.999   \n",
      "12                   هل لديك احذيه حريمي عند اصابع القدم  MSA\\n  0.999   \n",
      "13                     هل ستواجهني ايه صعوبات في الجمارك  MSA\\n  0.999   \n",
      "14     لا هل لي ان اقترح اخذ غرفه ذات سريرين للاستخدا...  MSA\\n  1.000   \n",
      "15                       اريد تمويج متقارب لشعري من فضلك  MSA\\n  0.999   \n",
      "16                                          هي تريد بيره  MSA\\n  0.666   \n",
      "17                                   صباح الخير كيف حالك  MSA\\n  0.407   \n",
      "18     اسف انني بحاجه الي ان اطلع علي اي مستند لتحديد...  MSA\\n  1.000   \n",
      "19                                     اي نوع من الملابس  MSA\\n  0.326   \n",
      "20            هذا بارد الي حد ما هل يمكن ان تقوم بتسخينه  MSA\\n  0.999   \n",
      "21          انا بحاله جديه لكن هل تمانع في الاتصال بصديق  MSA\\n  0.999   \n",
      "22                                    من هو افضل صديق لك  MSA\\n  0.995   \n",
      "23     يوم الصحه الرياضيه هو يوم العاشر من اكتوبر وهو...  MSA\\n  1.000   \n",
      "24                         هل تريد ان افحص موتور السياره  MSA\\n  0.999   \n",
      "25                 هل لك ان تساعدني في الوصول الي حقيبتي  MSA\\n  0.998   \n",
      "26                         هل ستقوم بعمل الترتيبات مقدما  MSA\\n  0.996   \n",
      "27                             تقدم الي المساعده شكرا لك  MSA\\n  0.413   \n",
      "28                   عندما تصل هناك اسال شخص اخر من فضلك  MSA\\n  0.999   \n",
      "29                     اريد الامتداد اربع وعشرون من فضلك  MSA\\n  0.999   \n",
      "...                                                  ...    ...    ...   \n",
      "41570                           حنرفعك لي كرسيك من هنايا  TRI\\n  0.999   \n",
      "41571                               عفوا نقدر نسالك سءال  TRI\\n  0.761   \n",
      "41572                                           اظني ايه  TRI\\n  0.735   \n",
      "41573                          خير فرحان اني تلاقيت معاك  TRI\\n  0.998   \n",
      "41574                   نبي نشحن الشنطه لي طوكيو من فضلك  TRI\\n  0.995   \n",
      "41575                         تقدر توريني واحد اكبر شويه  TRI\\n  0.705   \n",
      "41576       اظني الحوايج المصنوعات في اليابان مخيطات خير  TRI\\n  0.999   \n",
      "41577                                كان احسنلك انك توقف  TRI\\n  0.980   \n",
      "41578          تقدر تكتبلي العنوان و رقم التلفون لو سمحت  TRI\\n  0.510   \n",
      "41579                                               مسخن  TRI\\n  0.113   \n",
      "41580                              اما مطار اني ماشي منه  TRI\\n  0.990   \n",
      "41581                        ما عطوكش النموذج في الطياره  TRI\\n  0.999   \n",
      "41582                                           نبي مكوه  TRI\\n  0.287   \n",
      "41583           صحيتو علي مشاركتكم لينا بافكاركم المميزه  TRI\\n  0.999   \n",
      "41584       اني نمشي نتزلج مرتين في الشهر في موسم التزلج  TRI\\n  0.999   \n",
      "41585  اني انجبت اول ابريل سنه الف و تسعميه وخمسه و خ...  TRI\\n  0.999   \n",
      "41586          هل هدا الباص المضبوط الي يرفع لوسط البلاد  TRI\\n  0.999   \n",
      "41587                                  بقداش الريحه هادي  TRI\\n  0.999   \n",
      "41588                                            لي شيني  TRI\\n  0.823   \n",
      "41589                    وين نقدر نمشي للاجراءات الرسميه  TRI\\n  0.744   \n",
      "41590                            من فضلك كلم سياره اسعاف  TRI\\n  0.998   \n",
      "41591                                           كيف جايه  TRI\\n  0.194   \n",
      "41592                              مافيش ماكله ما نحبهاش  TRI\\n  0.999   \n",
      "41593  يوم الدستور او كيمبو كينيبي يوم تلاته مايو يوم...  TRI\\n  0.999   \n",
      "41594          هل الجوله السياحيه تشمل زياره لي برج سيرز  TRI\\n  0.993   \n",
      "41595                         هل في مكتبه قريبه من هنايا  TRI\\n  0.998   \n",
      "41596                 اني بنخش مدرسه صيفه في جامعه هاواي  TRI\\n  0.999   \n",
      "41597                              من وين خاطمين حني توا  TRI\\n  0.999   \n",
      "41598                          زوز كبار و عيل تلاته سنين  TRI\\n  0.999   \n",
      "41599                                  في حمام في المحطه  TRI\\n  0.978   \n",
      "\n",
      "          model  \n",
      "0      msa-char  \n",
      "1      msa-char  \n",
      "2      msa-char  \n",
      "3      mus-char  \n",
      "4      msa-char  \n",
      "5      msa-char  \n",
      "6      msa-char  \n",
      "7      msa-char  \n",
      "8      msa-char  \n",
      "9      jed-char  \n",
      "10     msa-char  \n",
      "11     msa-char  \n",
      "12     msa-char  \n",
      "13     msa-char  \n",
      "14     msa-char  \n",
      "15     msa-char  \n",
      "16     msa-char  \n",
      "17     msa-char  \n",
      "18     msa-char  \n",
      "19     mus-char  \n",
      "20     msa-char  \n",
      "21     msa-char  \n",
      "22     msa-char  \n",
      "23     msa-char  \n",
      "24     msa-char  \n",
      "25     msa-char  \n",
      "26     msa-char  \n",
      "27     riy-char  \n",
      "28     msa-char  \n",
      "29     msa-char  \n",
      "...         ...  \n",
      "41570  tri-char  \n",
      "41571  ben-char  \n",
      "41572  msa-char  \n",
      "41573  tri-char  \n",
      "41574  tri-char  \n",
      "41575  tri-char  \n",
      "41576  tri-char  \n",
      "41577  tri-char  \n",
      "41578  jer-char  \n",
      "41579  ale-char  \n",
      "41580  tri-char  \n",
      "41581  tri-char  \n",
      "41582  asw-char  \n",
      "41583  tri-char  \n",
      "41584  tri-char  \n",
      "41585  tri-char  \n",
      "41586  tri-char  \n",
      "41587  tri-char  \n",
      "41588  tri-char  \n",
      "41589  tri-char  \n",
      "41590  tri-char  \n",
      "41591  tri-char  \n",
      "41592  tri-char  \n",
      "41593  tri-char  \n",
      "41594  tri-char  \n",
      "41595  tri-char  \n",
      "41596  tri-char  \n",
      "41597  tri-char  \n",
      "41598  tri-char  \n",
      "41599  doh-char  \n",
      "\n",
      "[41600 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('feature1', Pipeline(memory=None,\n",
       "     steps=[('selector', ItemSelector(key='phrase')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', inp...ansformer_weights=None)), ('clasifier', MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Feature3(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        res=[{'model':x[0]} \n",
    "            for x in X[['model']].values]\n",
    "        return res\n",
    "pipeline = Pipeline([\n",
    "\n",
    "      ('features', FeatureUnion(\n",
    "           transformer_list =[\n",
    "\n",
    "       ('feature1', Pipeline([\n",
    "                    ('selector', ItemSelector(key='phrase')),\n",
    "('tfidf', TfidfVectorizer(tokenizer=stemming_tokenizer,analyzer='word',ngram_range=(1,1))),\n",
    "        ])),\n",
    "       ('feature2', Pipeline([\n",
    "            ('stats',TextStats()), ('vect', DictVectorizer())\n",
    "\n",
    "       ]))\n",
    "           ]\n",
    ")),\n",
    "\n",
    "    ('clasifier', MultinomialNB(alpha=0.5,fit_prior=True, class_prior=None)),\n",
    "                 ])\n",
    "\n",
    "pipeline.fit(df, df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour evaluation \n",
    "listscoretest=[ ]\n",
    "listmodeltest=[]\n",
    "for i in list(dftest['phrase']):\n",
    "    listscoretest.append(scoremodel(i)[1])\n",
    "    listmodeltest.append(scoremodel(i)[0])\n",
    "dftest['score'] = listscoretest\n",
    "dftest['model'] = listmodeltest\n",
    "#data2 =  {'phrase':listtest, 'score':listscoretest}\n",
    "#dftest2 =  pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8534615384615385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ALE\n",
      "       0.86      0.81      0.83       213\n",
      "        ALG\n",
      "       0.93      0.98      0.95       190\n",
      "        ALX\n",
      "       0.78      0.82      0.79       190\n",
      "        AMM\n",
      "       0.80      0.80      0.80       199\n",
      "        ASW\n",
      "       0.87      0.76      0.81       228\n",
      "        BAG\n",
      "       0.89      0.71      0.79       249\n",
      "        BAS\n",
      "       0.82      0.89      0.85       185\n",
      "        BEI\n",
      "       0.87      0.90      0.88       194\n",
      "        BEN\n",
      "       0.85      0.90      0.87       189\n",
      "        CAI\n",
      "       0.80      0.85      0.82       186\n",
      "        DAM\n",
      "       0.77      0.84      0.80       183\n",
      "        DOH\n",
      "       0.85      0.72      0.78       238\n",
      "        FES\n",
      "       0.90      0.92      0.91       196\n",
      "        JED\n",
      "       0.87      0.83      0.85       210\n",
      "        JER\n",
      "       0.80      0.78      0.79       206\n",
      "        KHA\n",
      "       0.86      0.89      0.88       195\n",
      "        MOS\n",
      "       0.91      0.93      0.92       195\n",
      "        MSA\n",
      "       0.93      0.87      0.90       215\n",
      "        MUS\n",
      "       0.84      0.88      0.86       191\n",
      "        RAB\n",
      "       0.94      0.94      0.94       200\n",
      "        RIY\n",
      "       0.78      0.87      0.82       179\n",
      "        SAL\n",
      "       0.71      0.87      0.78       164\n",
      "        SAN\n",
      "       0.88      0.94      0.91       188\n",
      "        SFX\n",
      "       0.92      0.90      0.91       204\n",
      "        TRI\n",
      "       0.90      0.82      0.86       220\n",
      "        TUN\n",
      "       0.88      0.91      0.89       193\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      5200\n",
      "   macro avg       0.85      0.86      0.85      5200\n",
      "weighted avg       0.86      0.85      0.85      5200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ytest = pipeline.predict(dftest)\n",
    "print('Accuracy = ',np.mean(ytest==dftest['label']))\n",
    "print(classification_report(ytest,dftest['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corp = {\\'ale-char\\':0, \\'asw-char\\':0, \\'msa-char\\':0, \\'sal-char\\':0, \\'jed-char\\':0, \\'bag-char\\':0, \\'san-char\\':0,\\n        \\'kha-char\\':0, \\'rab-char\\':0, \\'mus-char\\':0, \\'jer-char\\':0, \\'bas-char\\':0, \\'tri-char\\':0, \\'alg-char\\':0, \\n        \\'riy-char\\':0, \\'amm-char\\':0, \\'tun-char\\':0, \\'fes-char\\':0, \\'ben-char\\':0, \\'alx-char\\':0, \\'sfx-char\\':0,\\n        \\'dam-char\\':0, \\'bei-char\\':0, \\'cai-char\\':0, \\'mos-char\\':0, \\'doh-char\\':0}\\n\\ndef scoremodel_fethi(sentence):\\n    dialects = corp.keys()\\n    for did in dialects:\\n        model = kenlm.LanguageModel(\\'/media/sameh/data/CORPUS/LM-corpus/modelm/\\' + did + \".binary\")\\n        corp[did] = model.score(sentence)\\n    return corp\\nfor i in listtrain :\\n    scoremodel_fethi(i)\\n    corp[\\'phrase\\'] = i\\ndfunion = pd.DataFrame(corp)\\nprint(dfunion.shape)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create pour chaque phrase leur score par différent modéle \n",
    "import subprocess, os\n",
    "corpdir = os.path.dirname(\"/media/sameh/data/CORPUS/LM-corpus/modelm/*.binary \")\n",
    "for root,dirs,files in os.walk(corpdir):\n",
    "    for file in files:\n",
    "        lissc=[]\n",
    "        code = file[:8]\n",
    "        for i in listtrain:\n",
    "            lissc.append(language(i,code))            \n",
    "        df[code]=lissc\n",
    "\n",
    "\"\"\"corp = {'ale-char':0, 'asw-char':0, 'msa-char':0, 'sal-char':0, 'jed-char':0, 'bag-char':0, 'san-char':0,\n",
    "        'kha-char':0, 'rab-char':0, 'mus-char':0, 'jer-char':0, 'bas-char':0, 'tri-char':0, 'alg-char':0, \n",
    "        'riy-char':0, 'amm-char':0, 'tun-char':0, 'fes-char':0, 'ben-char':0, 'alx-char':0, 'sfx-char':0,\n",
    "        'dam-char':0, 'bei-char':0, 'cai-char':0, 'mos-char':0, 'doh-char':0}\n",
    "\n",
    "def scoremodel_fethi(sentence):\n",
    "    dialects = corp.keys()\n",
    "    for did in dialects:\n",
    "        model = kenlm.LanguageModel('/media/sameh/data/CORPUS/LM-corpus/modelm/' + did + \".binary\")\n",
    "        corp[did] = model.score(sentence)\n",
    "    return corp\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
