{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALE', 'ALG', 'ALX', 'AMM', 'ASW', 'BAG', 'BAS', 'BEI', 'BEN', 'CAI', 'DAM', 'DOH', 'FES', 'JED', 'JER', 'KHA', 'MOS', 'MSA', 'MUS', 'RAB', 'RIY', 'SAL', 'SAN', 'SFX', 'TRI', 'TUN']\n"
     ]
    }
   ],
   "source": [
    "#phase de prétraitement \n",
    "corpus = load_files('D:/CORPUS/sys-train-26/', encoding = 'utf-8',decode_error='ignore')\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | \n",
    "                             َ    | \n",
    "                             ً    | \n",
    "                             ُ    | \n",
    "                             ٌ    | \n",
    "                             ِ    | \n",
    "                             ٍ    | \n",
    "                             ْ    | \n",
    "                             ـ     \n",
    "                         \"\"\", re.VERBOSE)\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "list=[]\n",
    "\n",
    "#supprime les diactritics \n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "#supprime les signes des ponctuations \n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', arabic_punctuations)\n",
    "    return text.translate(translator)\n",
    "\n",
    "#Supprimer les nombres \n",
    "def remove_numbers(text):\n",
    "    regex = re.compile(r\"(\\d|[\\u0660\\u0661\\u0662\\u0663\\u0664\\u0665\\u0666\\u0667\\u0668\\u0669])+\")\n",
    "    return re.sub(regex, ' ', text)\n",
    "\n",
    "#supprimer les noms non arabe \n",
    "def remove_non_arabic_words(text):\n",
    "    return ' '.join([word for word in text.split() if not re.findall(\n",
    "        r'[^\\s\\u0621\\u0622\\u0623\\u0624\\u0625\\u0626\\u0627\\u0628\\u0629\\u062A\\u062B\\u062C\\u062D\\u062E\\u062F\\u0630\\u0631\\u0632\\u0633\\u0634\\u0635\\u0636\\u0637\\u0638\\u0639\\u063A\\u0640\\u0641\\u0642\\u0643\\u0644\\u0645\\u0646\\u0647\\u0648\\u0649\\u064A]',\n",
    "        word)])\n",
    "\n",
    "for data in corpus.data:\n",
    "    data = remove_diacritics(data)\n",
    "    data = remove_punctuations(data)\n",
    "    data = remove_numbers(data)\n",
    "    data = remove_non_arabic_words(data)\n",
    "    list.append(data)\n",
    "#print(list)\n",
    "#Tokenizing text with scikit-learn\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3))\n",
    "X_train_counts = count_vect.fit_transform(list)\n",
    "print(corpus.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#From occurrences to frequencies\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training a classifier\n",
    "clf = MultinomialNB().fit(X_train_tfidf, corpus.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]\n",
    "\n",
    "#Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(tokenizer=stemming_tokenizer)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB(alpha=0.5)) ])\n",
    "text_clf.fit(list, corpus.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluation de performance sur donnée de test \n",
    "Dtest = load_files('D:/CORPUS/MADAR-Shared-Task-Subtask-1/',encoding='utf-8',decode_error='ignore')\n",
    "listtest=[]\n",
    "for data in Dtest.data:\n",
    "    data = remove_diacritics(data)\n",
    "    data = remove_punctuations(data)\n",
    "    data = remove_numbers(data)\n",
    "    data = remove_non_arabic_words(data)\n",
    "    listtest.append(data)\n",
    "predicted = text_clf.predict(listtest)\n",
    "print('Accuracy = ',np.mean(predicted == Dtest.target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ALE       1.00      1.00      1.00         2\n",
      "         ALG       1.00      1.00      1.00         3\n",
      "         ALX       1.00      1.00      1.00         1\n",
      "         AMM       1.00      1.00      1.00         1\n",
      "         ASW       1.00      1.00      1.00         1\n",
      "         BAG       1.00      1.00      1.00         1\n",
      "         BAS       1.00      1.00      1.00         1\n",
      "         BEI       1.00      1.00      1.00         1\n",
      "         BEN       1.00      1.00      1.00         1\n",
      "         CAI       1.00      1.00      1.00         1\n",
      "         DAM       1.00      1.00      1.00         1\n",
      "         DOH       1.00      1.00      1.00         1\n",
      "         FES       0.50      1.00      0.67         1\n",
      "         JED       1.00      1.00      1.00         1\n",
      "         JER       1.00      1.00      1.00         1\n",
      "         KHA       1.00      1.00      1.00         1\n",
      "         MOS       1.00      1.00      1.00         1\n",
      "         MSA       1.00      1.00      1.00         1\n",
      "         MUS       1.00      1.00      1.00         1\n",
      "         RAB       0.00      0.00      0.00         1\n",
      "         RIY       1.00      1.00      1.00         1\n",
      "         SAL       1.00      1.00      1.00         1\n",
      "         SAN       1.00      1.00      1.00         1\n",
      "         SFX       0.50      1.00      0.67         1\n",
      "         TRI       1.00      1.00      1.00         1\n",
      "         TUN       1.00      0.50      0.67         2\n",
      "\n",
      "   micro avg       0.93      0.93      0.93        30\n",
      "   macro avg       0.92      0.94      0.92        30\n",
      "weighted avg       0.93      0.93      0.92        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Mesure d'evaluation \n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "print(metrics.classification_report(Dtest.target, predicted, target_names=Dtest.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
